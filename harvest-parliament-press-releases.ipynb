{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Harvest parliament press releases from Trove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Trove includes more than 380,000 press releases, speeches, and interview transcripts issued by Australian federal politicians and saved by the Parliamentary Library. You can view them in the Trove web interface by searching for [nuc:\"APAR:PR\" in the books & libraries category](http://trove.nla.gov.au/search/category/books?keyword=nuc%3A%22APAR%3APR%22).\n",
    "\n",
    "This notebook shows you how to harvest both metadata and full text from a search of the parliamentary press releases. The metadata is available from Trove, but to get the full text we have to go back to the Parliamentary Library's database, ParlInfo. The code in this notebook updates [my original GitHub repository](https://github.com/wragge/trove-parliament-pressreleases).\n",
    "\n",
    "There are two main steps:\n",
    "\n",
    "* Use the Trove API to search for specific keywords within the press releases and harvest metadata from the results. This gives us urls that we can use to get the text of the press releases from ParlInfo.\n",
    "* Use the harvested urls to retrieve the press release from ParlInfo. The text of each release is extracted from the HTML page and saved as a plain text file.\n",
    "\n",
    "Sometimes multiple press releases can be grouped together as 'works' in Trove. This is because Trove thinks that they're versions of the same thing. However, these versions are not always identical, and sometimes Trove has grouped press releases together incorrectly. To make sure that we harvest as many individual press releases as possible, the code below unpacks any versions contained within a 'work' and turns them into individual records.\n",
    "\n",
    "It looks like the earlier documents have been OCRd and the results are quite variable. If you follow the `fulltext_url` link you should be able to view a PDF version for comparison.\n",
    "\n",
    "It also seems that some documents only have a PDF version and not any OCRd text. These documents will be ignored by the `save_texts()` function, so you might end up with fewer texts than records.\n",
    "\n",
    "The copyright statement attached to each record in Trove reads:\n",
    "\n",
    "> Copyright remains with the copyright holder. Contact the Australian Copyright Council for further information on your rights and responsibilities.\n",
    "\n",
    "So depending on what you want to do with them, you might need to contact individual copyright holders for permission.\n",
    "\n",
    "## Duplicates and false positives\n",
    "\n",
    "As noted Trove sometimes groups different press releases together as a single `work`. This seems to happen when press releases share a title and creator – for example, if an MP issues a press release titled 'Anzac Day' every year, these might be grouped as a single work. As noted above, all the different versions will be harvested by default. However, because search is operating at the work level, it's entirely possible that some of the grouped versions won't actually contain the search term you're looking for. To exclude these, we need to examine the text of each version individually to see if they match.\n",
    "\n",
    "There will also be press releases that have exactly the same text content, both within and across works. For example, when a press release is issued both by a Minister and their department, or when MPs disseminate press releases issued by their party.\n",
    "\n",
    "To make it easier to deal with these two issues, I've added some post-harvest processing steps to:\n",
    "\n",
    "* remove records where the text content of the press release doesn't include any of the search terms (you'll need to adjust this to meet your needs)\n",
    "* add a `hash` column that represents the text content of a press release – this can be used to identify duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## An example – politicians talking about 'immigrants' and 'refugees'\n",
    "\n",
    "I've used this notebook to update an example dataset relating to refugees that I first generated in December 2017. It's been created by searching for the terms 'immigrant', 'asylum seeker', 'boat people', 'illegal arrivals', and 'boat arrivals' amongst the press releases. The exact query used is:\n",
    "\n",
    "```\n",
    "nuc:\"APAR:PR\" AND (\"illegal arrival\" OR text:\"immigrant\" OR text:\"immigrants\" OR \"asylum seeker\" OR \"boat people\" OR refugee OR \"boat arrivals\")\n",
    "```\n",
    "\n",
    "You can view the [results of this query on Trove](https://trove.nla.gov.au/search/category/books?keyword=nuc%3A%22APAR%3APR%22%20AND%20%28%22illegal%20arrival%22%20OR%20text%3A%22immigrant%22%20OR%20text%3A%22immigrants%22%20OR%20%22asylum%20seeker%22%20OR%20%22boat%20people%22%20OR%20refugee%20OR%20%22boat%20arrivals%22%29).\n",
    "\n",
    "See [Press releases relating to refugees](https://glam-workbench.net/trove-government/trove-parliament-press-releases-refugees/) for the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import requests_cache\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from slugify import slugify\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "s = requests_cache.CachedSession()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[502, 503, 504])\n",
    "s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Set your options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below you need to insert your search query and your Trove API key. You can get a Trove API key by [following these instructions](https://help.nla.gov.au/trove/building-with-trove/api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your Trove API key\n",
    "API_KEY = \"YOUR API KEY\"\n",
    "\n",
    "# Use api key value from environment variables if it is available\n",
    "if os.getenv(\"TROVE_API_KEY\"):\n",
    "    API_KEY = os.getenv(\"TROVE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search query can be anything you would enter in the Trove search box. As you can see from the examples below it can include phrases, exact phrases, and boolean operators (`AND`, `OR`, and `NOT`).\n",
    "\n",
    "You can change `output_dir` to save the results to a specific directory on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Insert your query between the single quotes.\n",
    "query = \"ufo\"\n",
    "# Examples:\n",
    "# query = '(\"illegal arrival\" OR text:\"immigrant\" OR text:\"immigrants\" OR \"asylum seeker\" OR \"boat people\" OR refugee OR \"boat arrivals\")'\n",
    "# query = \"(COVID OR coronavirus)\"\n",
    "\n",
    "# You don't have to change this\n",
    "output_dir = \"press-releases\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Define some functions to do the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_total_results(params):\n",
    "    \"\"\"\n",
    "    Get the total number of results for a search.\n",
    "    \"\"\"\n",
    "    these_params = params.copy()\n",
    "    these_params[\"n\"] = 0\n",
    "    response = s.get(\"https://api.trove.nla.gov.au/v3/result\", params=these_params)\n",
    "    data = response.json()\n",
    "    return int(data[\"category\"][0][\"records\"][\"total\"])\n",
    "\n",
    "\n",
    "def get_fulltext_url(links):\n",
    "    \"\"\"\n",
    "    Loop through the identifiers to find a link to the digital version of the journal.\n",
    "    \"\"\"\n",
    "    url = None\n",
    "    for link in links:\n",
    "        if link[\"linktype\"] == \"fulltext\":\n",
    "            url = link[\"value\"]\n",
    "            break\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_source(version):\n",
    "    \"\"\"\n",
    "    Get the metadata source of a version.\n",
    "    \"\"\"\n",
    "    if \"metadataSource\" in version:\n",
    "        try:\n",
    "            source = version[\"metadataSource\"][\"value\"]\n",
    "        except TypeError:\n",
    "            try:\n",
    "                source = version[\"metadataSource\"]\n",
    "            except TypeError:\n",
    "                print(version)\n",
    "\n",
    "        except KeyError:\n",
    "            source = None\n",
    "    else:\n",
    "        source = None\n",
    "    return source\n",
    "\n",
    "\n",
    "def get_value(record, field, keys=[\"value\"]):\n",
    "    \"\"\"\n",
    "    Get the values of a field.\n",
    "    Some fields are lists of dicts, if so use the `key` to get the value.\n",
    "    \"\"\"\n",
    "    value = record.get(field, [])\n",
    "    if value and isinstance(value[0], dict):\n",
    "        for key in keys:\n",
    "            try:\n",
    "                return [re.sub(r\"\\s+\", \" \", v[key]) for v in value]\n",
    "            except KeyError:\n",
    "                pass\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "def merge_values(record, fields, keys=[\"value\"]):\n",
    "    \"\"\"\n",
    "    Merges values from multiple fields, removing any duplicates.\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    for field in fields:\n",
    "        values += get_value(record, field, keys)\n",
    "    # Remove duplicates and None value\n",
    "    return list(set([v for v in values if v is not None]))\n",
    "\n",
    "\n",
    "def flatten_values(record, field, key=\"type\"):\n",
    "    \"\"\"\n",
    "    If a field has a value and type, return the values as strings with this format: 'type: value'\n",
    "    \"\"\"\n",
    "    flattened = []\n",
    "    values = record.get(field, [])\n",
    "    for value in values:\n",
    "        if key in value:\n",
    "            flattened.append(f\"{value[key]}: {value['value']}\")\n",
    "        else:\n",
    "            flattened.append(value[\"value\"])\n",
    "    return flattened\n",
    "\n",
    "\n",
    "def harvest_prs(query):\n",
    "    \"\"\"\n",
    "    Harvest details of parliamentary press releases using the Trove API.\n",
    "    This function saves the 'version' level records individually (these are grouped under 'works').\n",
    "    \"\"\"\n",
    "    # Define parameters for the search -- you could change this of course\n",
    "    # The nuc:\"APAR:PR\" limits the results to the Parliamentary Press Releases\n",
    "    params = {\n",
    "        \"q\": f'nuc:\"APAR:PR\" AND ({query})',\n",
    "        \"category\": \"all\",\n",
    "        \"n\": 100,\n",
    "        \"bulkHarvest\": \"true\",\n",
    "        \"encoding\": \"json\",\n",
    "        \"include\": \"workVersions\",\n",
    "        \"l-availability\": \"y\",\n",
    "    }\n",
    "    start = \"*\"\n",
    "    total = get_total_results(params)\n",
    "    url = \"http://api.trove.nla.gov.au/v3/result\"\n",
    "    with tqdm(total=total) as pbar:\n",
    "        with Path(f\"press-releases-{slugify(query)}.ndjson\").open(\"w\") as ndjson_out:\n",
    "            while start:\n",
    "                params[\"s\"] = start\n",
    "                response = s.get(url, params=params)\n",
    "                data = response.json()\n",
    "                # If there's a startNext value then we get it to request the next page of results\n",
    "                try:\n",
    "                    start = data[\"category\"][0][\"records\"][\"nextStart\"]\n",
    "                except KeyError:\n",
    "                    start = None\n",
    "                items = data[\"category\"][0][\"records\"][\"item\"]\n",
    "                for item in items:\n",
    "                    for category, record in item.items():\n",
    "                        if category == \"work\":\n",
    "                            # Different records can be grouped within works as versions.\n",
    "                            # So we're going to extract each version as a separate record.\n",
    "                            for version in record[\"version\"]:\n",
    "                                # Sometimes there are even versions grouped together in a version... ¯\\_(ツ)_/¯\n",
    "                                # We need to extract their ids from a single string\n",
    "                                ids = version[\"id\"].split()\n",
    "                                # Loop through versions in versions.\n",
    "                                for index, sub_version in enumerate(version[\"record\"]):\n",
    "                                    source = get_source(sub_version)\n",
    "                                    if source == \"APAR:PR\":\n",
    "                                        metadata = sub_version[\"metadata\"][\"dc\"]\n",
    "                                        work = {\n",
    "                                            \"version_id\": ids[index],\n",
    "                                            \"work_id\": record[\"id\"],\n",
    "                                            \"work_type\": record.get(\"type\", []),\n",
    "                                            \"title\": get_value(metadata, \"title\"),\n",
    "                                            \"contributor\": merge_values(\n",
    "                                                metadata,\n",
    "                                                [\"creator\", \"contributor\"],\n",
    "                                                [\"value\", \"name\"],\n",
    "                                            ),\n",
    "                                            \"date\": merge_values(\n",
    "                                                metadata, [\"date\", \"issued\"]\n",
    "                                            ),\n",
    "                                            \"description\": get_value(\n",
    "                                                metadata, \"description\"\n",
    "                                            ),\n",
    "                                            # Using merge here because I've noticed some duplicate values\n",
    "                                            \"type\": merge_values(metadata, [\"type\"]),\n",
    "                                            \"format\": get_value(metadata, \"format\"),\n",
    "                                            \"language\": get_value(metadata, \"language\"),\n",
    "                                            \"extent\": get_value(metadata, \"extent\"),\n",
    "                                            \"rights\": merge_values(\n",
    "                                                metadata, [\"rights\", \"licenseRef\"]\n",
    "                                            ),\n",
    "                                            \"subject\": merge_values(\n",
    "                                                metadata, [\"subject\"]\n",
    "                                            ),\n",
    "                                            # Flattened type/value\n",
    "                                            \"is_part_of\": flatten_values(\n",
    "                                                metadata, \"isPartOf\"\n",
    "                                            ),\n",
    "                                            \"fulltext_url\": get_fulltext_url(\n",
    "                                                metadata[\"identifier\"]\n",
    "                                            ),\n",
    "                                        }\n",
    "                                        ndjson_out.write(\n",
    "                                            f\"{json.dumps(work, ensure_ascii=False)}\\n\"\n",
    "                                        )\n",
    "                pbar.update(100)\n",
    "\n",
    "\n",
    "def save_texts(query, output_dir=\"press-releases\"):\n",
    "    \"\"\"\n",
    "    Get the text of press releases in the ParlInfo db.\n",
    "    This function uses urls harvested from Trove to request press releases from Parlinfo.\n",
    "    Text is extracted from the HTML files and saved as individual text files.\n",
    "    \"\"\"\n",
    "    input = Path(f\"press-releases-{slugify(query)}.ndjson\")\n",
    "    output_path = Path(output_dir, f\"press-releases-{slugify(query)}\", \"text\")\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    # ParlInfo requires a user-agent\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/119.0\",\n",
    "    }\n",
    "    total = sum(1 for _ in open(input))\n",
    "    with input.open(\"r\") as ndjson_in:\n",
    "        for line in tqdm(ndjson_in, total=total):\n",
    "            record = json.loads(line)\n",
    "            contributor = slugify(\n",
    "                record[\"contributor\"][0] if record[\"contributor\"] else \"unknown\"\n",
    "            )\n",
    "            filename = f\"{record['date'][0]}-{contributor}-{record['version_id']}.txt\"\n",
    "            file_path = Path(output_path, filename)\n",
    "            # Only save files we haven't saved before\n",
    "            if not file_path.exists():\n",
    "                # Get the Parlinfo web page\n",
    "                response = requests.get(record[\"fulltext_url\"], headers=headers)\n",
    "                # Parse web page in Beautiful Soup\n",
    "                soup = BeautifulSoup(response.text, \"lxml\")\n",
    "                content = soup.find(\"div\", class_=\"box\")\n",
    "                # If we find some text on the web page then save it.\n",
    "                if content:\n",
    "                    # Open file\n",
    "                    # print 'Saving file...'\n",
    "                    with open(file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "                        # Get the contents of each paragraph and write it to the file\n",
    "                        for para in content.find_all(\"p\"):\n",
    "                            text_file.write(\"{}\\n\\n\".format(para.get_text().strip()))\n",
    "                else:\n",
    "                    # No content could be an error at APH\n",
    "                    print(response.url)\n",
    "                time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Harvest the metadata!\n",
    "\n",
    "Running the cell below will harvest details of all the press releases matching our query using the Trove API. The results will be saved in the `records` variable for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "records = harvest_prs(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Download the text files\n",
    "\n",
    "The details we've harvested from the Trove API include a url that points to the full text of the press release in the ParlInfo database. Now we can loop through all those urls, saving the text of the press releases. Sometimes files don't download on first attempt. It's worth running this cell multiple times to see if some additional texts are downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_texts(query, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Convert to a dataframe\n",
    "\n",
    "To make it easier to manipulate the harvested metadata, we'll convert the `ndjson` file to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version_id</th>\n",
       "      <th>work_id</th>\n",
       "      <th>work_type</th>\n",
       "      <th>title</th>\n",
       "      <th>contributor</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>type</th>\n",
       "      <th>format</th>\n",
       "      <th>language</th>\n",
       "      <th>extent</th>\n",
       "      <th>rights</th>\n",
       "      <th>subject</th>\n",
       "      <th>is_part_of</th>\n",
       "      <th>fulltext_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>211350983</td>\n",
       "      <td>193080997</td>\n",
       "      <td>[Article/Other article, Article]</td>\n",
       "      <td>[Transcript of joint press conference with New...</td>\n",
       "      <td>[Rudd, Kevin, Mccully, Murray]</td>\n",
       "      <td>[2011-03-26]</td>\n",
       "      <td>[A press release issued by a member of the Aus...</td>\n",
       "      <td>[Press Release]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[eng]</td>\n",
       "      <td>[5p.]</td>\n",
       "      <td>[Copyright remains with the copyright holder. ...</td>\n",
       "      <td>[visit to New Zealand, Syria, New Zealand dome...</td>\n",
       "      <td>[Press releases database, Australian Parliamen...</td>\n",
       "      <td>http://parlinfo.aph.gov.au/parlInfo/search/dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211352306</td>\n",
       "      <td>193082252</td>\n",
       "      <td>[Sound/Other sound, Sound]</td>\n",
       "      <td>[Transcript of interview with the Hot Breakfas...</td>\n",
       "      <td>[the Hot Breakfast Team, Abbott, Tony]</td>\n",
       "      <td>[2011-05-06]</td>\n",
       "      <td>[A press release captured for archiving in the...</td>\n",
       "      <td>[Press Release, Broadcast transcript]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[eng]</td>\n",
       "      <td>[7p.]</td>\n",
       "      <td>[Copyright remains with the copyright holder. ...</td>\n",
       "      <td>[Julia Gillard's carbon tax]</td>\n",
       "      <td>[Triple M, Adelaide, Press releases database, ...</td>\n",
       "      <td>http://parlinfo.aph.gov.au/parlInfo/search/dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>211479325</td>\n",
       "      <td>193193664</td>\n",
       "      <td>[Article/Other article, Article]</td>\n",
       "      <td>[St. Clair 'the truth is out there']</td>\n",
       "      <td>[St Clair, Stuart, National Party of Australia]</td>\n",
       "      <td>[1999-12-09]</td>\n",
       "      <td>[A press release captured for archiving in the...</td>\n",
       "      <td>[Press Release]</td>\n",
       "      <td>[Online Text]</td>\n",
       "      <td>[eng]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Copyright remains with the copyright holder. ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Press releases database, Australian Parliamen...</td>\n",
       "      <td>http://parlinfo.aph.gov.au/parlInfo/search/dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>213729661</td>\n",
       "      <td>195167931</td>\n",
       "      <td>[Article/Other article, Article]</td>\n",
       "      <td>[Address to Federal Council - Perth]</td>\n",
       "      <td>[Fraser, Malcolm]</td>\n",
       "      <td>[1979-04-22]</td>\n",
       "      <td>[A press release issued by a member of the Aus...</td>\n",
       "      <td>[Press Release, Speech]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[eng]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Copyright remains with the copyright holder. ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Press releases database, Australian Parliamen...</td>\n",
       "      <td>http://parlinfo.aph.gov.au/parlInfo/search/dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>213769918</td>\n",
       "      <td>195229143</td>\n",
       "      <td>[Article/Other article, Article]</td>\n",
       "      <td>[Information Technology- a Critical View]</td>\n",
       "      <td>[Jones, Barry]</td>\n",
       "      <td>[1981-08-22]</td>\n",
       "      <td>[A press release issued by a member of the Aus...</td>\n",
       "      <td>[Press Release, Speech]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[eng]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Copyright remains with the copyright holder. ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Press releases database, Australian Parliamen...</td>\n",
       "      <td>http://parlinfo.aph.gov.au/parlInfo/search/dis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version_id    work_id                         work_type  \\\n",
       "0   211350983  193080997  [Article/Other article, Article]   \n",
       "1   211352306  193082252        [Sound/Other sound, Sound]   \n",
       "2   211479325  193193664  [Article/Other article, Article]   \n",
       "3   213729661  195167931  [Article/Other article, Article]   \n",
       "4   213769918  195229143  [Article/Other article, Article]   \n",
       "\n",
       "                                               title  \\\n",
       "0  [Transcript of joint press conference with New...   \n",
       "1  [Transcript of interview with the Hot Breakfas...   \n",
       "2               [St. Clair 'the truth is out there']   \n",
       "3               [Address to Federal Council - Perth]   \n",
       "4          [Information Technology- a Critical View]   \n",
       "\n",
       "                                       contributor          date  \\\n",
       "0                   [Rudd, Kevin, Mccully, Murray]  [2011-03-26]   \n",
       "1           [the Hot Breakfast Team, Abbott, Tony]  [2011-05-06]   \n",
       "2  [St Clair, Stuart, National Party of Australia]  [1999-12-09]   \n",
       "3                                [Fraser, Malcolm]  [1979-04-22]   \n",
       "4                                   [Jones, Barry]  [1981-08-22]   \n",
       "\n",
       "                                         description  \\\n",
       "0  [A press release issued by a member of the Aus...   \n",
       "1  [A press release captured for archiving in the...   \n",
       "2  [A press release captured for archiving in the...   \n",
       "3  [A press release issued by a member of the Aus...   \n",
       "4  [A press release issued by a member of the Aus...   \n",
       "\n",
       "                                    type         format language extent  \\\n",
       "0                        [Press Release]             []    [eng]  [5p.]   \n",
       "1  [Press Release, Broadcast transcript]             []    [eng]  [7p.]   \n",
       "2                        [Press Release]  [Online Text]    [eng]     []   \n",
       "3                [Press Release, Speech]             []    [eng]     []   \n",
       "4                [Press Release, Speech]             []    [eng]     []   \n",
       "\n",
       "                                              rights  \\\n",
       "0  [Copyright remains with the copyright holder. ...   \n",
       "1  [Copyright remains with the copyright holder. ...   \n",
       "2  [Copyright remains with the copyright holder. ...   \n",
       "3  [Copyright remains with the copyright holder. ...   \n",
       "4  [Copyright remains with the copyright holder. ...   \n",
       "\n",
       "                                             subject  \\\n",
       "0  [visit to New Zealand, Syria, New Zealand dome...   \n",
       "1                       [Julia Gillard's carbon tax]   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                          is_part_of  \\\n",
       "0  [Press releases database, Australian Parliamen...   \n",
       "1  [Triple M, Adelaide, Press releases database, ...   \n",
       "2  [Press releases database, Australian Parliamen...   \n",
       "3  [Press releases database, Australian Parliamen...   \n",
       "4  [Press releases database, Australian Parliamen...   \n",
       "\n",
       "                                        fulltext_url  \n",
       "0  http://parlinfo.aph.gov.au/parlInfo/search/dis...  \n",
       "1  http://parlinfo.aph.gov.au/parlInfo/search/dis...  \n",
       "2  http://parlinfo.aph.gov.au/parlInfo/search/dis...  \n",
       "3  http://parlinfo.aph.gov.au/parlInfo/search/dis...  \n",
       "4  http://parlinfo.aph.gov.au/parlInfo/search/dis...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(f\"press-releases-{slugify(query)}.ndjson\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Note that the number of records in the harvested data might be different to the number of search results. This is because we've unpacked versions that had been combined into a single work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many records\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Sometimes it's not possible to download the text from a press release. Let's see how many text files were actually downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sum(\n",
    "        1\n",
    "        for _ in Path(output_dir, f\"press-releases-{slugify(query)}\", \"text\").glob(\n",
    "            \"*.txt\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Removing non-matches\n",
    "\n",
    "**The cell below will delete records, so don't run it unless you understand what it's doing!**\n",
    "\n",
    "As noted above, some of the press releases might not actually match our search. The cell below uses regular expressions to run a very basic check of the harvested text files to see if they contain the desired search terms. You will need to adjust `pattern` to suit your desired search results. In particular, you'll need to consider the amount of fuzziness you might expect in your search results and whether that will be captured by the regular expression pattern. If this is a problem, it might be better to use something like [`fuzzysearch`](https://pypi.org/project/fuzzysearch/) to do the comparisons.\n",
    "\n",
    "If the desired search terms are not found in a text file, the corresponding Trove record is removed from the results dataframe, and the text file is deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change this!\n",
    "# pattern = r\"(covid|coronavirus)\"\n",
    "pattern = rf\"\\b{query}\\b\"\n",
    "\n",
    "for text_file in Path(output_dir, f\"press-releases-{slugify(query)}\", \"text\").glob(\n",
    "    \"*.txt\"\n",
    "):\n",
    "    # Are our search terms in the file?\n",
    "    if re.findall(pattern, text_file.read_text().lower()) == []:\n",
    "        # Get the version id\n",
    "        version_id = re.search(r\"\\-(\\d+)\\.txt\", text_file.name).group(1)\n",
    "        # Remove the record with that version_id from the dataset\n",
    "        df = df.loc[df[\"version_id\"] != int(version_id)]\n",
    "        # Delete the text file\n",
    "        text_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "How many records do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sum(\n",
    "        1\n",
    "        for _ in Path(output_dir, f\"press-releases-{slugify(query)}\", \"text\").glob(\n",
    "            \"*.txt\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Find press releases with duplicate content\n",
    "\n",
    "As noted above, there might be press releases that have the same content, but different metadata (eg title or creator). To make these duplicates easy to identify, this cell adds a `hash` column to the dataset. The `hash` value is a short string representation of each record's associated text file. If two records have the same `hash` value, then the contents of the press releases will be the same.\n",
    "\n",
    "If you want, you can use this column to drop duplicates from the dataset. On the other hand, if you're interested in seeing how press releases are disseminated, you might want to group records by their `hash` values and compare the metadata within each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hash(version_id):\n",
    "    try:\n",
    "        text_file = next(\n",
    "            Path(output_dir, f\"press-releases-{slugify(query)}\", \"text\").glob(\n",
    "                f\"*-{version_id}.txt\"\n",
    "            )\n",
    "        )\n",
    "        hashed = hashlib.sha1(text_file.read_text().encode()).hexdigest()\n",
    "    except StopIteration:\n",
    "        print(version_id)\n",
    "        hashed = None\n",
    "    return hashed\n",
    "\n",
    "\n",
    "df[\"hash\"] = df[\"version_id\"].apply(get_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "How many unique press releases are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"hash\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Save the dataset\n",
    "\n",
    "Let's save the dataset as a CSV file for download.\n",
    "\n",
    "Many of the columns in the dataset contain multiple values in a list. Before we convert the dataset to CSV, we'll convert these lists to strings, using the `|` character to separate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_lists(column):\n",
    "    try:\n",
    "        return column.apply(lambda x: \"|\".join(x) if isinstance(x, list) else x)\n",
    "    except AttributeError:\n",
    "        return column\n",
    "\n",
    "\n",
    "df = df.apply(merge_lists)\n",
    "\n",
    "# Add a Trove link to each work/version\n",
    "df[\"trove_url\"] = df.apply(\n",
    "    lambda x: f\"https://trove.nla.gov.au/work/{x['work_id']}/version/{x['version_id']}\",\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save the metadata as a CSV-formatted dataset. The dataset and the downloaded text files will be in the `press-releases` directory, inside a sub directory named according to your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Save the data as a CSV file\n",
    "output_path = Path(output_dir, f\"press-releases-{slugify(query)}\")\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "df[\n",
    "    [\n",
    "        \"title\",\n",
    "        \"contributor\",\n",
    "        \"date\",\n",
    "        \"description\",\n",
    "        \"type\",\n",
    "        \"format\",\n",
    "        \"work_type\",\n",
    "        \"language\",\n",
    "        \"extent\",\n",
    "        \"rights\",\n",
    "        \"subject\",\n",
    "        \"is_part_of\",\n",
    "        \"fulltext_url\",\n",
    "        \"trove_url\",\n",
    "        \"work_id\",\n",
    "        \"version_id\",\n",
    "        \"hash\",\n",
    "    ]\n",
    "].to_csv(Path(output_path, \"results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "----\n",
    "\n",
    "Created by [Tim Sherratt](https://timsherratt.org/) for the [GLAM Workbench](https://glam-workbench.net/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rocrate": {
   "action": [
    {
     "description": "Trove includes more than 380,000 press releases, speeches, and interview transcripts issued by Australian federal politicians and saved by the Parliamentary Library. This dataset contains metadata and full text of items from the press releases collection that include the term 'refugees' (or a number of related terms).\n\nThe dataset was created in two stages. First, Trove was searched for relevant resources, and metadata from each search result was saved in a CSV file. Then links to ParlInfo contained within the metadata were used to download the full text.\n\nKnown issues:\n\n- some items had no downloadable text available in Parlinfo, so the number of text files is less than the number of metadata records\n- some items have different metadata but the same text, for example when a press release is issued by both a political party and an individual member\n- because of the way Trove groups records, it's possible some of the records don't include any of the search terms.",
     "isPartOf": "https://github.com/GLAM-Workbench/trove-parliament-press-releases-refugees/",
     "local_path": "press-releases/press-releases-illegal-arrival-or-text-immigrant-or-text-immigrants-or-asylum-seeker-or-boat-people-or-refugee-or-boat-arrivals",
     "mainEntityOfPage": "https://glam-workbench.net/trove-government/trove-parliament-press-releases-refugees",
     "query": "nuc:\"APAR:PR\" AND (\"illegal arrival\" OR text:\"immigrant\" OR text:\"immigrants\" OR \"asylum seeker\" OR \"boat people\" OR refugee OR \"boat arrivals\")",
     "result": [
      {
       "description": "Contains individual text files, each containing the contents of an item from the press releases collection. Files are named using the following metadata fields: `date`, `contributor`, and `version_id`. For example: `1960-01-22-casey-richard-213589278.txt`.",
       "license": "http://rightsstatements.org/vocab/CNE/1.0/",
       "url": "https://github.com/GLAM-Workbench/trove-parliament-press-releases-refugees/tree/main/text/"
      },
      {
       "license": "https://creativecommons.org/publicdomain/zero/1.0/",
       "url": "https://github.com/GLAM-Workbench/trove-parliament-press-releases-refugees/raw/main/results.csv"
      }
     ],
     "workExample": [
      {
       "name": "Explore using Datasette",
       "url": "https://glam-workbench.net/datasette-lite/?csv=https://raw.githubusercontent.com/GLAM-Workbench/trove-parliament-press-releases-refugees/main/results.csv&fts=title,contributor,subject"
      }
     ]
    },
    {
     "description": "Trove includes more than 380,000 press releases, speeches, and interview transcripts issued by Australian federal politicians and saved by the Parliamentary Library. This dataset contains metadata and full text of items from the press releases collection that include the term 'covid' or 'coronavirus'.\n\nThe dataset was created in two stages. First, Trove was searched for relevant resources, and metadata from each search result was saved in a CSV file. Then links to ParlInfo contained within the metadata were used to download the full text.\n\nKnown issues:\n\n- some items had no downloadable text available in Parlinfo, so the number of text files is less than the number of metadata records\n- some items have different metadata but the same text, for example when a press release is issued by both a political party and an individual member\n- because of the way Trove groups records, it's possible some of the records don't include any of the search terms.",
     "isPartOf": "https://github.com/GLAM-Workbench/trove-parliament-press-releases-covid/",
     "local_path": "press-releases/press-releases-covid-or-coronavirus",
     "mainEntityOfPage": "https://glam-workbench.net/trove-government/trove-parliament-press-releases-covid",
     "query": "nuc:\"APAR:PR\" AND (covid OR coronavirus)",
     "result": [
      {
       "description": "Contains individual text files, each containing the contents of an item from the press releases collection. Files are named using the following metadata fields: `date`, `contributor`, and `version_id`. For example: `1960-01-22-casey-richard-213589278.txt`.",
       "license": "http://rightsstatements.org/vocab/CNE/1.0/",
       "url": "https://github.com/GLAM-Workbench/trove-parliament-press-releases-covid/tree/main/text/"
      },
      {
       "license": "https://creativecommons.org/publicdomain/zero/1.0/",
       "url": "https://github.com/GLAM-Workbench/trove-parliament-press-releases-covid/raw/main/results.csv"
      }
     ],
     "workExample": [
      {
       "name": "Explore using Datasette",
       "url": "https://glam-workbench.net/datasette-lite/?csv=https://raw.githubusercontent.com/GLAM-Workbench/trove-parliament-press-releases-covid/main/results.csv&fts=title,contributor,subject"
      }
     ]
    }
   ],
   "author": [
    {
     "mainEntityOfPage": "https://timsherratt.au",
     "name": "Sherratt, Tim",
     "orcid": "https://orcid.org/0000-0001-7956-4498"
    }
   ],
   "description": "Trove includes more than 380,000 press releases, speeches, and interview transcripts issued by Australian federal politicians and saved by the Parliamentary Library. This notebook shows you how to harvest both metadata and full text from a search of the parliamentary press releases. The metadata is available from Trove, but to get the full text we have to go back to the Parliamentary Library's database, ParlInfo.",
   "mainEntityOfPage": "https://glam-workbench.net/trove-government/harvest-parliament-press-releases/",
   "name": "Harvest parliament press releases from Trove"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
